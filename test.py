import cv2
import os
import time
import numpy as np
from datetime import datetime
from lib.camera import Camera
from lib.yolo_and_sam import YOLOSegmentator
from lib.mask2pose import mask2pose, draw_pose_axes
from lib.dobot import DobotRobot
from cv2 import aruco

def detect_aruco_pose(camera,image):
    """
    æ£€æµ‹ArUcoæ¿å­çš„ä½å§¿
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ
        
    è¿”å›:
        success: æ˜¯å¦æ£€æµ‹æˆåŠŸ
        rvec: æ—‹è½¬å‘é‡
        tvec: å¹³ç§»å‘é‡
        corners: æ£€æµ‹åˆ°çš„è§’ç‚¹
        ids: æ£€æµ‹åˆ°çš„ID
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_APRILTAG_36H11)
    aruco_params = cv2.aruco.DetectorParameters()
    # ArUcoæ¿å­å‚æ•° (8x8cm)
    marker_size = 0.08  # 8cm = 0.08m
    board_size = (4, 4)  # 4x4çš„ArUcoæ¿å­
    
    camera_matrix = camera.get_camera_matrix('color')
    dist_coeffs = camera.get_distortion_coeffs('color')
    
    # æ£€æµ‹ArUcoæ ‡è®°
    corners, ids, _ = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)
    
    if ids is None or len(ids) < 1:  # è‡³å°‘éœ€è¦1ä¸ªæ ‡è®°
        return False, None, None, None, None
    
    # å¯¹äºå•ä¸ªæ ‡è®°ï¼Œä½¿ç”¨estimatePoseSingleMarkers
    if len(ids) == 1:
        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(
            corners, marker_size, camera_matrix, dist_coeffs
        )
        rvec = rvecs[0][0]
        tvec = tvecs[0][0]
        success = True
    else:
        # å¯¹äºå¤šä¸ªæ ‡è®°ï¼Œæš‚æ—¶åªå¤„ç†ç¬¬ä¸€ä¸ªæ ‡è®°
        # TODO: å®ç°çœŸæ­£çš„æ¿å­ä½å§¿ä¼°è®¡
        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(
            corners[:1], marker_size, camera_matrix, dist_coeffs
        )
        rvec = rvecs[0][0]
        tvec = tvecs[0][0]
        success = True

    
    aruco_to_cam = np.eye(4)
    aruco_to_cam[:3, :3] = cv2.Rodrigues(rvec)[0]
    aruco_to_cam[:3, 3] = tvec.flatten()
    
    return aruco_to_cam



def main():

    cam = Camera(camera_model='D405')  # åˆå§‹åŒ–ç›¸æœº
    robot = DobotRobot(robot_ip='192.168.5.1')  # åˆå§‹åŒ–æœºæ¢°è‡‚
    # robot.r_inter.StartDrag()
    # return 0
    
    
    
    # # è¯»å–åŸå§‹å›¾åƒ
    color_image_path = 'test/color.png'
    depth_image_path = 'test/depth.png'
    color_image = cv2.imread(color_image_path)
    depth_image = cv2.imread(depth_image_path, cv2.IMREAD_UNCHANGED)
    # color_image, depth_image = cam.get_frames()
    # cv2.imwrite('test/color.png',color_image)
    # cv2.imwrite('test/depth.png',depth_image)
    # return 0

    # ç¡®ä¿æ·±åº¦å›¾æ˜¯äºŒç»´çš„
    if len(depth_image.shape) == 3:
        print(f"  æ·±åº¦å›¾æ˜¯3é€šé“,å–ç¬¬ä¸€ä¸ªé€šé“")
        depth_image = depth_image[:, :, 0]
    
    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶ä½¿ç”¨æ­£ç¡®çš„æ·±åº¦æ¯”ä¾‹è½¬æ¢ä¸ºç±³
    # RealSense D405çš„æ·±åº¦æ¯”ä¾‹æ˜¯0.0001 (ä¸æ˜¯0.001!)
    # æ‰€ä»¥è¦é™¤ä»¥10000 (ä¸æ˜¯1000)
    depth_scale = 0.0001  # ä»ç›¸æœºä¿¡æ¯ä¸­è·å–
    depth_image = depth_image.astype(np.float32) * depth_scale
    
    
    T_cam2base = np.eye(4)  # å‡è®¾ç›¸æœºä½å§¿å·²çŸ¥

    categories_to_find = ['cup']
    segmentator = YOLOSegmentator()
    
    # æ‰“å°å›¾åƒä¿¡æ¯ç”¨äºè°ƒè¯•
    print(f"\nğŸ” å›¾åƒä¿¡æ¯:")
    print(f"  å›¾åƒå°ºå¯¸: {color_image.shape}")
    print(f"  æ•°æ®ç±»å‹: {color_image.dtype}")
    print(f"  æ£€æµ‹ç±»åˆ«: {categories_to_find}")
    
    # ä½¿ç”¨è°ƒæ•´åçš„å›¾åƒè¿›è¡Œæ£€æµ‹
    result = segmentator.detect_and_segment_all(
        image=color_image,
        categories=categories_to_find,
        save_result=False,
        conf=0.1  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä»0.1é™åˆ°0.051
    )

    # result['objects']æ˜¯ä¸€ä¸ªå­—å…¸çš„listï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªç‰©ä½“åŒ…å«ç±»åˆ«ã€ç½®ä¿¡åº¦ã€è¾¹ç•Œæ¡†å’Œæ©ç 

    intrinsics = cam.get_camera_matrix()
    
    # æ£€æŸ¥æ£€æµ‹æ˜¯å¦æˆåŠŸ
    if not result['success']:
        print("âŒ æ£€æµ‹å¤±è´¥ï¼šæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“")
        print("å¯èƒ½çš„åŸå› ï¼š")
        print("1. å›¾åƒä¸­æ²¡æœ‰ 'cup' ç‰©ä½“")
        print("2. ç‰©ä½“ç½®ä¿¡åº¦å¤ªä½ï¼ˆå½“å‰é˜ˆå€¼: 0.1ï¼‰")
        print("3. å›¾åƒè´¨é‡é—®é¢˜")
        return
    
    if 'objects' not in result or len(result['objects']) == 0:
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“")
        return
    
    for idx, obj in enumerate(result['objects']):
        print(f"\nç‰©ä½“ {idx + 1}:")
        print(f"  ç±»åˆ«: {obj['class']}")
        print(f"  ç½®ä¿¡åº¦: {obj['confidence']:.2f}")
        print(f"  è¾¹ç•Œæ¡†: {obj['bbox_xyxy']}")
        
        # å¦‚æœæœ‰æ©ç ï¼Œè®¡ç®—ä½å§¿
        if obj['mask'] is not None:
            print(f"  æ©ç å°ºå¯¸: {obj['mask'].shape}")
            print(f"  å½©è‰²å›¾åƒå°ºå¯¸: {color_image.shape}")
            print(f"  æ·±åº¦å›¾åƒå°ºå¯¸: {depth_image.shape}")
            
            pose, T_object2cam = mask2pose(
                mask=obj['mask'],
                depth_image=depth_image,
                color_image=color_image,
                intrinsics=intrinsics,
                T_cam2base=T_cam2base,
                object_class=obj['class']
            )
            
            if pose is not None:
                print(f"   ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹ä½ç½®: [{pose[0]:.3f}, {pose[1]:.3f}, {pose[2]:.3f}] ç±³")
                print(f"   ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹å§¿æ€: [{pose[3]:.1f}Â°, {pose[4]:.1f}Â°, {pose[5]:.1f}Â°]")
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“
            else:
                print(f"   âŒ ä½å§¿ä¼°è®¡å¤±è´¥")
        else:
            print(f"   âš ï¸ æœªè·å–åˆ°æ©ç ")
    
    
    

    calibration_file = 'best_hand_eye_calibration.npy'
    if not os.path.exists(calibration_file):
        raise FileNotFoundError(f"æ‰‹çœ¼æ ‡å®šæ–‡ä»¶ä¸å­˜åœ¨: {calibration_file}")
    
    hand_eye_matrix = np.load(calibration_file)
            
    # aruco_to_cam = detect_aruco_pose(cam,color_image)
    
    target_to_cam = T_object2cam
    
    pose_matrix =  robot.get_pose_matrix() @ hand_eye_matrix @ target_to_cam
    
    # robot.moveL(pose_matrix)
    print("ç›®æ ‡ç‰©ä½“ç›¸å¯¹äºæœºæ¢°è‡‚åŸºåº§çš„ä½å§¿çŸ©é˜µ:\n", pose_matrix)

    # init_joint_state = np.array([-90.0, 0.0, -90.0, 0.0, 90.0, 90.0])  # left arm
    # robot.moveJ(init_joint_state)
    # pose_matrix = np.array([[    0.26074  ,  -0.10167  ,  -0.96004  ,  -0.56438],
    #                         [   -0.96455  ,  0.014468 ,    -0.2635  ,  -0.18789],
    #                         [   0.040679  ,  0.99471  , -0.094291   ,   0.5605],
    #                         [          0  ,         0 ,          0   ,        1]])
#     pose_matrix = np.array( [[    0.99875    0.031514   -0.038725     0.40659]
#  [    0.04272    -0.94085     0.33611      0.5097]
#  [  -0.025842    -0.33734    -0.94103     0.45623]
#  [          0           0           0           1]])
    draw_pose_axes(color_image, intrinsics, T_object2cam)
    from scipy.spatial.transform import Rotation as R
    rx, ry, rz = R.from_matrix(pose_matrix[:3, :3]).as_euler('xyz', degrees=True)
    x,y,z = np.array(pose_matrix[:3, 3]) *1000.0
    ry = ry + 90 
    robot.moveL(np.array([x,y,z,rx,ry,rz]))
    # print(f"ArUcoæ¿ç›¸å¯¹äºæœºæ¢°è‡‚åŸºåº§ä½ç½® (mm): X={position[0]*1000:.2f}, Y={position[1]*1000:.2f}, Z={position[2]*1000:.2f}\n")
    # print(f"ArUcoæ¿ç›¸å¯¹äºæœºæ¢°è‡‚åŸºåº§å˜æ¢çŸ©é˜µ:\n{pose_matrix}")



if __name__ == "__main__":
    main()