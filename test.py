import cv2
import os
import time
import numpy as np
from datetime import datetime
from lib.camera import Camera
from lib.yolo_and_sam import YOLOSegmentator
from lib.mask2pose import mask2pose, visualize_result
from lib.dobot import DobotRobot

def main():

    cam = Camera(camera_model='D405')  # åˆå§‹åŒ–ç›¸æœº
    robot = DobotRobot(robot_ip='192.168.5.1')  # åˆå§‹åŒ–æœºæ¢°è‡‚
    # robot.r_inter.StartDrag()
    # return 0
    
    
    # # è¯»å–åŸå§‹å›¾åƒ
    # color_image_path = 'test/color.png'
    # depth_image_path = 'test/depth.png'
    # color_image = cv2.imread(color_image_path)
    # depth_image = cv2.imread(depth_image_path, cv2.IMREAD_UNCHANGED)
    color_image, depth_image = cam.get_frames()

    
    # # ç¡®ä¿æ·±åº¦å›¾æ˜¯äºŒç»´çš„
    # if len(depth_image.shape) == 3:
    #     print(f"  æ·±åº¦å›¾æ˜¯3é€šé“,å–ç¬¬ä¸€ä¸ªé€šé“")
    #     depth_image = depth_image[:, :, 0]
    
    # # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶ä½¿ç”¨æ­£ç¡®çš„æ·±åº¦æ¯”ä¾‹è½¬æ¢ä¸ºç±³
    # # RealSense D405çš„æ·±åº¦æ¯”ä¾‹æ˜¯0.0001 (ä¸æ˜¯0.001!)
    # # æ‰€ä»¥è¦é™¤ä»¥10000 (ä¸æ˜¯1000)
    # depth_scale = 0.0001  # ä»ç›¸æœºä¿¡æ¯ä¸­è·å–
    # depth_image = depth_image.astype(np.float32) * depth_scale
    
    
    # T_cam2base = np.eye(4)  # å‡è®¾ç›¸æœºä½å§¿å·²çŸ¥

    # categories_to_find = ['cup']
    # segmentator = YOLOSegmentator()
    
    # # ä½¿ç”¨è°ƒæ•´åçš„å›¾åƒè¿›è¡Œæ£€æµ‹
    # result = segmentator.detect_and_segment_all(
    #     image=color_image,
    #     categories=categories_to_find,
    #     save_result=True
    # )
    # print("Detection and segmentation result:", result)



    # # result['objects']æ˜¯ä¸€ä¸ªå­—å…¸çš„listï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªç‰©ä½“åŒ…å«ç±»åˆ«ã€ç½®ä¿¡åº¦ã€è¾¹ç•Œæ¡†å’Œæ©ç 
    # pose = None
    # T_cam2object = None
    
    # for idx, obj in enumerate(result['objects']):
    #     print(f"\nç‰©ä½“ {idx + 1}:")
    #     print(f"  ç±»åˆ«: {obj['class']}")
    #     print(f"  ç½®ä¿¡åº¦: {obj['confidence']:.2f}")
    #     print(f"  è¾¹ç•Œæ¡†: {obj['bbox_xyxy']}")
        
    #     # å¦‚æœæœ‰æ©ç ï¼Œè®¡ç®—ä½å§¿
    #     if obj['mask'] is not None:
    #         print(f"  æ©ç å°ºå¯¸: {obj['mask'].shape}")
    #         print(f"  å½©è‰²å›¾åƒå°ºå¯¸: {color_image.shape}")
    #         print(f"  æ·±åº¦å›¾åƒå°ºå¯¸: {depth_image.shape}")
            
    #         pose, T_cam2object = mask2pose(
    #             mask=obj['mask'],
    #             depth_image=depth_image,
    #             color_image=color_image,
    #             intrinsics=cam.get_camera_matrix(),
    #             T_cam2base=T_cam2base,
    #             object_class=obj['class']
    #         )
            
    #         if pose is not None:
    #             print(f"   ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹ä½ç½®: [{pose[0]:.3f}, {pose[1]:.3f}, {pose[2]:.3f}] ç±³")
    #             print(f"   ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹å§¿æ€: [{pose[3]:.1f}Â°, {pose[4]:.1f}Â°, {pose[5]:.1f}Â°]")
    #             print(f"T:{T_cam2object}")
    #             break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“
    #         else:
    #             print(f"   âŒ ä½å§¿ä¼°è®¡å¤±è´¥")
    #     else:
    #         print(f"   âš ï¸ æœªè·å–åˆ°æ©ç ")
    
    # ArUcoæ ‡è®°æ£€æµ‹å’Œä½å§¿ä¼°è®¡
    from cv2 import aruco
    
    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_APRILTAG_36H11)
    aruco_params = cv2.aruco.DetectorParameters()

    gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
    marker_corners, marker_ids, rejected = aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)

    # è·å–ç›¸æœºå†…å‚
    camera_matrix = cam.get_camera_matrix()
    distortion_coefficients = np.zeros(5)  # D405é€šå¸¸ç•¸å˜å¾ˆå°,å¯ä»¥è®¾ä¸º0
    
    markerLength = 0.08  # ArUcoæ ‡è®°çš„è¾¹é•¿,å•ä½:ç±³ (æ ¹æ®å®é™…æ ‡è®°å°ºå¯¸ä¿®æ”¹)
    
    T_cam2object = None
    pose = None
    
    # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„ marker
    if marker_ids is not None:
        print(f"\næ£€æµ‹åˆ° {len(marker_ids)} ä¸ªArUcoæ ‡è®°")
        
        rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(
            marker_corners, markerLength, camera_matrix, distortion_coefficients
        )
        
        for i in range(len(marker_ids)):
            marker_id = marker_ids[i][0]
            rvec = rvecs[i][0]  # æ—‹è½¬å‘é‡
            tvec = tvecs[i][0]  # å¹³ç§»å‘é‡
            
            print(f"\nArUcoæ ‡è®° ID: {marker_id}")
            print(f"  å¹³ç§»å‘é‡(ç›¸æœºåæ ‡ç³»): {tvec}")
            print(f"  æ—‹è½¬å‘é‡: {rvec}")
            
            # å°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            rotation_matrix, _ = cv2.Rodrigues(rvec)
            
            # æ„å»º4x4å˜æ¢çŸ©é˜µ
            T_cam2object = np.eye(4)
            T_cam2object[:3, :3] = rotation_matrix
            T_cam2object[:3, 3] = tvec
            T_object2cam = T_cam2object #hs
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’ç”¨äºæ˜¾ç¤º
            from scipy.spatial.transform import Rotation as R
            r = R.from_matrix(rotation_matrix)
            euler_angles = r.as_euler('xyz', degrees=True)
            
            # æ„å»ºposeæ ¼å¼ [x, y, z, rx, ry, rz]
            pose = [tvec[0], tvec[1], tvec[2], euler_angles[0], euler_angles[1], euler_angles[2]]
            
            print(f"  ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹ä½ç½®: [{tvec[0]:.3f}, {tvec[1]:.3f}, {tvec[2]:.3f}] ç±³")
            print(f"  ğŸ“ ç›¸æœºåæ ‡ç³»ä¸‹å§¿æ€: [{euler_angles[0]:.1f}Â°, {euler_angles[1]:.1f}Â°, {euler_angles[2]:.1f}Â°]")
            print(f"  T_cam2object:\n{T_cam2object}")
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶åæ ‡è½´
            # image_copy = color_image.copy()
            # cv2.drawFrameAxes(image_copy, camera_matrix, distortion_coefficients, rvec, tvec, 0.05)
            # cv2.imwrite(f'aruco_detected_id_{marker_id}.png', image_copy)
            # print(f"  å·²ä¿å­˜æ£€æµ‹ç»“æœå›¾åƒ: aruco_detected_id_{marker_id}.png")
            
            # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„æ ‡è®°
            break
    else:
        print("\nâŒ æœªæ£€æµ‹åˆ°ArUcoæ ‡è®°")     

    # åæ ‡å˜æ¢: ç›¸æœºåæ ‡ç³» -> åŸºåº§åæ ‡ç³»
    if pose is not None and T_cam2object is not None:
        T_cam2tool = np.array([
            [ 0.06012576,  0.99535858, -0.07514113, -0.08242748],
            [-0.99807343,  0.06110251,  0.01076621,  0.00971775],
            [ 0.01530755,  0.07434904,  0.99711479, -0.16371493],
            [ 0.        ,  0.        ,  0.        ,  1.        ]
        ])  # ç›¸æœºåˆ°å·¥å…·åæ ‡ç³»çš„å˜æ¢(é€šè¿‡æ‰‹çœ¼æ ‡å®šè·å¾—)
        
        T_tool2base = robot.get_pose_matrix()  # è·å–å½“å‰æœºæ¢°è‡‚æœ«ç«¯ä½å§¿
        # T_base2tool = np.linalg.inv(T_tool2base)
        
        # T_cam2object æ˜¯ç‰©ä½“åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿çŸ©é˜µ (4x4)
        # å˜æ¢é“¾: T_base2object = T_base2tool @ T_tool2cam @ T_cam2object
        # T_tool2cam = np.linalg.inv(T_cam2tool)
        # T_base2object = T_base2tool @ T_tool2cam @ T_cam2object
        
        T_object2base = T_object2cam @ T_cam2tool @ T_tool2base
        
        # æå–åŸºåº§åæ ‡ç³»ä¸‹çš„ä½ç½®
        position_base = T_object2base[:3, 3]
        
        # æå–åŸºåº§åæ ‡ç³»ä¸‹çš„å§¿æ€(æ¬§æ‹‰è§’)
        from scipy.spatial.transform import Rotation as R
        rotation_base = R.from_matrix(T_object2base[:3, :3])
        euler_base = rotation_base.as_euler('xyz', degrees=True)
        
        print(f"\nğŸ¤– åŸºåº§åæ ‡ç³»ä¸‹çš„ç›®æ ‡ä½å§¿:")
        print(f"   ä½ç½®: [{position_base[0]:.3f}, {position_base[1]:.3f}, {position_base[2]:.3f}] ç±³")
        print(f"   å§¿æ€: [{euler_base[0]:.1f}Â°, {euler_base[1]:.1f}Â°, {euler_base[2]:.1f}Â°]")
        
        # æ„å»ºç›®æ ‡ä½å§¿æ•°ç»„ [x, y, z, rx, ry, rz]
        target_pose = np.array([
            position_base[0], position_base[1], position_base[2],
            euler_base[0], euler_base[1], euler_base[2]
        ])
        
        # ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
        print(f"\nâ¡ï¸  ç§»åŠ¨æœºæ¢°è‡‚åˆ°ç›®æ ‡ä½ç½®...")
        joints= robot.r_inter.InverseSolution(*target_pose,1,1)
        print(joints)
        # robot.moveL(target_pose)
        print(f"âœ… ç§»åŠ¨å®Œæˆ!")
    else:
        print(f"\nâŒ æœªèƒ½è·å–æœ‰æ•ˆçš„ç‰©ä½“ä½å§¿ï¼Œæ— æ³•ç§»åŠ¨æœºæ¢°è‡‚")    
    

if __name__ == "__main__":
    main()