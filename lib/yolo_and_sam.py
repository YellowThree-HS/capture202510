import torch
from ultralytics import YOLOWorld, FastSAM, SAM
import numpy as np
import time
import os
import cv2
from PIL import Image
from functools import wraps


def timer(func):
    """
    è®¡æ—¶è£…é¥°å™¨ï¼šç»Ÿè®¡å‡½æ•°æ‰§è¡Œæ—¶é—´
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # è·å–å‡½æ•°å
        func_name = func.__name__
        print(f"â±ï¸  [{func_name}] took {elapsed_time:.3f} seconds")
        
        return result
    return wrapper


class YOLOSegmentator:
    def __init__(self, yolo_weights="weights/yolov8s-world.pt", sam_weights="weights/sam2.1_b.pt", use_sam2=True):
        """
        åˆå§‹åŒ– YOLO-World å’Œ SAM æ¨¡å‹ã€‚
        
        å‚æ•°:
            yolo_weights: YOLOæ¨¡å‹æƒé‡è·¯å¾„
            sam_weights: SAMæ¨¡å‹æƒé‡è·¯å¾„
            use_sam2: æ˜¯å¦ä½¿ç”¨SAM2.1ï¼Œå¦åˆ™ä½¿ç”¨FastSAM
        """
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        self.use_sam2 = use_sam2

        # --- æ­£ç¡®çš„ YOLO-World åˆå§‹åŒ–æµç¨‹ ---

        # 1. åŠ è½½æ¨¡å‹ (é»˜è®¤åœ¨ CPU ä¸Š)
        print("Loading YOLO-World model ...")
        self.det_model = YOLOWorld(yolo_weights)
        self.det_model.to(self.device)

        # åŠ è½½ SAM æ¨¡å‹
        if self.use_sam2:
            print("Loading SAM2.1 model...")
            self.seg_model = SAM(sam_weights)
        else:
            print("Loading FastSAM model...")
            self.seg_model = FastSAM(sam_weights)
            self.seg_model.to(self.device)

    @timer
    def detect(self, image, categories, output_dir="result", conf=0.1, imgsz=640, save_result=True):
        """
        ä½¿ç”¨ YOLO-World æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“
        
        å‚æ•°:
            image: å›¾åƒæ•°æ® (numpy æ•°ç»„æ ¼å¼)/å›¾åƒè·¯å¾„
            categories: è¦æ£€æµ‹çš„ç±»åˆ«åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: å›¾åƒå¤§å°
            
        è¿”å›:
            dict: {
                'success': bool,
                'det_bboxes': YOLO result object (åŒ…å«boxes, cls, confç­‰),
                'detection_path': str (ä¿å­˜çš„æ£€æµ‹ç»“æœè·¯å¾„)
            }
        """

        print("\n--- Running YOLO-World Detection ---")

        self.det_model.set_classes(categories)

        if type(image) == str:
            pass
        elif isinstance(image, list):
            image = Image.fromarray(image)
        #å¦‚æœpredictå¤šä¸ªimgï¼Œå¯¹äºæœ‰å¤šä¸ªimgçš„ç»“æœ.list
        det_results = self.det_model.predict(
            source=image,
            conf=conf,
            imgsz=imgsz,
            device=self.device,
            verbose=False
        )

        #det_resultsåŒ…å«äº†å¤§é‡çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬.boxes(xyxy,cls,conf),orig_img,names,speedç­‰ç­‰
        result = det_results[0]

        #é”™è¯¯æç¤º
        if len(result.boxes) == 0:
            print("No objects detected by YOLO-World.")
            return {'success': False, 'det_bboxes': None, 'detection_path': None}

        #ä¿å­˜ç»“æœ
        if save_result:
             # ä¿å­˜æ£€æµ‹ç»“æœï¼ˆå¸¦æ ‡ç­¾çš„æ£€æµ‹æ¡†ï¼‰
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            det_output_filename = os.path.join(output_dir, f"det_{os.path.basename(image) if isinstance(image, str) else f'{timestamp}.png'}")
            result.save(det_output_filename)
            print('YOLO-World results:')
            print(f"Detection result saved to {det_output_filename}")
            print(f"Found {len(result.boxes)} objects")

        result_dict = {
            'success': True,
            'det_bboxes': result.boxes,
            'detection_path': det_output_filename if save_result else None
        }

        return result_dict

    def _expand_bbox(self, bbox_gpu, expand_pixels, image):
        """
        æ‰©å±•æ£€æµ‹æ¡†è¾¹ç•Œ
        
        å‚æ•°:
            bbox_gpu: åŸå§‹è¾¹ç•Œæ¡†å¼ é‡ (æ ¼å¼ä¸º xyxy)
            expand_pixels: æ‰©å±•çš„åƒç´ æ•°
            image: å›¾åƒæ•°æ®ï¼Œç”¨äºè·å–å›¾åƒå°ºå¯¸
            
        è¿”å›:
            bbox_expanded: æ‰©å±•åçš„è¾¹ç•Œæ¡†å¼ é‡
        """
        import torch
        
        # è·å–å›¾åƒå°ºå¯¸
        if isinstance(image, str):
            # å¦‚æœæ˜¯å›¾åƒè·¯å¾„ï¼Œè¯»å–å›¾åƒè·å–å°ºå¯¸
            import cv2
            img = cv2.imread(image)
            h, w = img.shape[:2]
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„
            h, w = image.shape[:2]
        
        # å°†GPUå¼ é‡è½¬æ¢ä¸ºCPU numpyæ•°ç»„è¿›è¡Œå¤„ç†
        bbox_cpu = bbox_gpu.cpu().numpy()
        x1, y1, x2, y2 = bbox_cpu
        
        # æ‰©å±•è¾¹ç•Œæ¡†
        x1_expanded = max(0, x1 - expand_pixels)
        y1_expanded = max(0, y1 - expand_pixels)
        x2_expanded = min(w, x2 + expand_pixels)
        y2_expanded = min(h, y2 + expand_pixels)
        
        # è½¬æ¢å›GPUå¼ é‡
        bbox_expanded = torch.tensor([x1_expanded, y1_expanded, x2_expanded, y2_expanded], 
                                   device=bbox_gpu.device, dtype=bbox_gpu.dtype)
        
        print(f"   æ£€æµ‹æ¡†æ‰©å±•: [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}] -> [{x1_expanded:.0f}, {y1_expanded:.0f}, {x2_expanded:.0f}, {y2_expanded:.0f}]")
        
        return bbox_expanded

    def _draw_dashed_rectangle(self, img, pt1, pt2, color, thickness, dash_length=10):
        """
        ç»˜åˆ¶è™šçº¿çŸ©å½¢
        
        å‚æ•°:
            img: å›¾åƒ
            pt1: å·¦ä¸Šè§’ç‚¹ (x1, y1)
            pt2: å³ä¸‹è§’ç‚¹ (x2, y2)
            color: é¢œè‰²
            thickness: çº¿æ¡ç²—ç»†
            dash_length: è™šçº¿é•¿åº¦
        """
        x1, y1 = pt1
        x2, y2 = pt2
        
        # ç»˜åˆ¶å››æ¡è¾¹çš„è™šçº¿
        # ä¸Šè¾¹
        self._draw_dashed_line(img, (x1, y1), (x2, y1), color, thickness, dash_length)
        # ä¸‹è¾¹
        self._draw_dashed_line(img, (x1, y2), (x2, y2), color, thickness, dash_length)
        # å·¦è¾¹
        self._draw_dashed_line(img, (x1, y1), (x1, y2), color, thickness, dash_length)
        # å³è¾¹
        self._draw_dashed_line(img, (x2, y1), (x2, y2), color, thickness, dash_length)

    def _draw_dashed_line(self, img, pt1, pt2, color, thickness, dash_length):
        """
        ç»˜åˆ¶è™šçº¿
        
        å‚æ•°:
            img: å›¾åƒ
            pt1: èµ·ç‚¹ (x1, y1)
            pt2: ç»ˆç‚¹ (x2, y2)
            color: é¢œè‰²
            thickness: çº¿æ¡ç²—ç»†
            dash_length: è™šçº¿é•¿åº¦
        """
        x1, y1 = pt1
        x2, y2 = pt2
        
        # è®¡ç®—çº¿æ¡é•¿åº¦å’Œæ–¹å‘
        length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
        if length == 0:
            return
            
        # è®¡ç®—å•ä½æ–¹å‘å‘é‡
        dx = (x2 - x1) / length
        dy = (y2 - y1) / length
        
        # ç»˜åˆ¶è™šçº¿
        current_length = 0
        draw = True
        
        while current_length < length:
            # è®¡ç®—å½“å‰æ®µçš„èµ·ç‚¹å’Œç»ˆç‚¹
            start_x = int(x1 + current_length * dx)
            start_y = int(y1 + current_length * dy)
            
            end_length = min(current_length + dash_length, length)
            end_x = int(x1 + end_length * dx)
            end_y = int(y1 + end_length * dy)
            
            if draw:
                cv2.line(img, (start_x, start_y), (end_x, end_y), color, thickness)
            
            current_length = end_length
            draw = not draw


    @timer
    def segment(self, image, bbox_gpu, output_dir, save_result, expand_pixels=40):
        """
        ä½¿ç”¨ SAM å¯¹æŒ‡å®šè¾¹ç•Œæ¡†è¿›è¡Œåˆ†å‰²
        
        å‚æ•°:
            image: å›¾åƒæ•°æ® (numpy æ•°ç»„æ ¼å¼) | å›¾ç‰‡åœ°å€
            bbox_gpu: è¾¹ç•Œæ¡†å¼ é‡ (åœ¨GPUä¸Šï¼Œæ ¼å¼ä¸º xyxy)
            output_dir: è¾“å‡ºç›®å½•
            save_result: æ˜¯å¦ä¿å­˜åˆ†å‰²ç»“æœ
            expand_pixels: æ£€æµ‹æ¡†å‘å¤–æ‰©å±•çš„åƒç´ æ•°
            
        è¿”å›:
            dict: {
                'success': bool,
                'mask': np.ndarray (åˆ†å‰²æ©ç ),
                'segmentation_path': str (å¦‚æœä¿å­˜äº†ç»“æœ)
            }
        """
        
        # æ‰©å±•æ£€æµ‹æ¡†
        bbox_expanded = self._expand_bbox(bbox_gpu, expand_pixels, image)
        
        if self.use_sam2:
            # ä½¿ç”¨SAM2.1è¿›è¡Œåˆ†å‰² (ultralyticsæ¥å£)
            bbox_cpu = bbox_expanded.cpu().numpy()
            x1, y1, x2, y2 = bbox_cpu
            sam_results = self.seg_model(image, bboxes=[x1, y1, x2, y2])
        else:
            # ä½¿ç”¨FastSAMè¿›è¡Œåˆ†å‰²
            sam_results = self.seg_model(image, bboxes=bbox_expanded.unsqueeze(0))


        # æå–æ©ç æ•°æ®
        mask = None
        if sam_results[0].masks is not None and len(sam_results[0].masks) > 0:
            mask = sam_results[0].masks.data[0].cpu().numpy().astype(np.uint8)


        # ä¿å­˜åˆ†å‰²ç»“æœ
        if save_result:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            seg_output_filename = os.path.join(output_dir, f"seg_{os.path.basename(image) if isinstance(image, str) else f'{timestamp}.png'}")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„saveæ–¹æ³•
            sam_results[0].save(seg_output_filename)
            model_name = "SAM2.1" if self.use_sam2 else "FastSAM"
            print(f'{model_name} results:')
            print(f"Segmentation result saved to {seg_output_filename}")

        result_dict = {
            'success': True,
            'mask': mask,
            'segmentation_path': seg_output_filename if save_result else None
        }

        return result_dict

    def detect_and_segment(self, image, categories, output_dir="./result", conf=0.1, imgsz=640, save_result=True):
        """
        å…ˆç”¨ YOLO-World æ£€æµ‹ï¼Œç„¶åç”¨ FastSAM åˆ†å‰²ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡ã€‚
        **åªåˆ†å‰²å…¨å±€ç½®ä¿¡åº¦æœ€é«˜çš„1ä¸ªç‰©ä½“
        """
        if save_result:
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
        # æ­¥éª¤1: æ£€æµ‹
        det_result = self.detect(image, categories, output_dir, conf, imgsz)
        
        if not det_result['success']:
            return det_result
        
        det_bboxes = det_result['det_bboxes']
        

        # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ªç‰©ä½“-------------------------------------------------------
        max_index_gpu = det_bboxes.conf.argmax()
        best_box_gpu = det_bboxes.xyxy[max_index_gpu]
        best_class_id_gpu = det_bboxes.cls[max_index_gpu]
        best_confidence_gpu = det_bboxes.conf[max_index_gpu]

        # å°†ç»“æœç§»è‡³ CPU
        best_box_xyxy = best_box_gpu.cpu().numpy().astype(int)
        best_class_name = self.det_model.names[int(best_class_id_gpu.cpu())]
        best_confidence = float(best_confidence_gpu.cpu())

        #-------------------------------------------------------------------------------------------

        print(f"\nBest object: {best_class_name} (confidence: {best_confidence:.2f})")

        # æ­¥éª¤2: åˆ†å‰²
        seg_result = self.segment(image, best_box_gpu, output_dir, save_result=save_result, expand_pixels=40)

        return {
            'success': True,
            'best_object': {
                'class': best_class_name,
                'confidence': best_confidence,
                'bbox_xyxy': best_box_xyxy.tolist()
            },
            'mask': seg_result['mask'],
            'detection_path': det_result['detection_path'] if save_result else None,
            'segmentation_path': seg_result.get('segmentation_path') if save_result else None
        }
    
    def detect_and_segment_all(self, image, categories, output_dir="./result", conf=0.1, imgsz=640, save_result=False, 
                               multi_instance_classes=None, multi_conf_threshold=0.3, multi_max_count=3):
        """
        æ£€æµ‹å¹¶åˆ†å‰²æ‰€æœ‰æŒ‡å®šç±»åˆ«çš„ç‰©ä½“
        
        å‚æ•°:
            image: å›¾åƒè·¯å¾„æˆ–æ•°ç»„
            categories: è¦æ£€æµ‹çš„ç±»åˆ«åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            conf: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: å›¾åƒå¤§å°
            save_result: æ˜¯å¦ä¿å­˜ç»“æœ
            multi_instance_classes: å…è®¸ä¿ç•™å¤šä¸ªå®ä¾‹çš„ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚ ['bowl']ï¼‰
            multi_conf_threshold: å¤šå®ä¾‹ç±»åˆ«çš„ç½®ä¿¡åº¦é˜ˆå€¼
            multi_max_count: å¤šå®ä¾‹ç±»åˆ«çš„æœ€å¤§ä¿ç•™æ•°é‡
        
        è¿”å›:
            dict: {
                'success': bool,
                'objects': [
                    {
                        'class': str,
                        'confidence': float,
                        'bbox_xyxy': list,
                        'mask': np.ndarray
                    },
                    ...
                ]
            }
        """
        if save_result:
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
        
        if multi_instance_classes is None:
            multi_instance_classes = []
            
        # æ­¥éª¤1: æ£€æµ‹
        det_result = self.detect(image, categories, output_dir, conf, imgsz)
        
        if not det_result['success']:
            return det_result
        
        det_bboxes = det_result['det_bboxes']

        # æ­¥éª¤2: ç­›é€‰ç‰©ä½“
        print(f"\nFound {len(det_bboxes)} objects, filtering...")

        # åˆ›å»º (idx, class_name, confidence) çš„åˆ—è¡¨å¹¶æŒ‰ç±»åˆ«åˆ†ç»„
        objects_by_class = {}
        for idx in range(len(det_bboxes)):
            info = {
                'idx': idx,
                'class_name': self.det_model.names[int(det_bboxes.cls[idx].cpu())],
                'confidence': float(det_bboxes.conf[idx].cpu())
            }
            class_name = info['class_name']
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯å¤šå®ä¾‹ç±»åˆ«
            if class_name in multi_instance_classes:
                # å¤šå®ä¾‹ç±»åˆ«ï¼šä¿ç•™æ‰€æœ‰æ»¡è¶³é˜ˆå€¼çš„ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
                if class_name not in objects_by_class:
                    objects_by_class[class_name] = []
                if info['confidence'] >= multi_conf_threshold:
                    objects_by_class[class_name].append(info)
            else:
                # å•å®ä¾‹ç±»åˆ«ï¼šåªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
                if class_name not in objects_by_class or info['confidence'] > objects_by_class[class_name]['confidence']:
                    objects_by_class[class_name] = info
        
        # å¤„ç†å¤šå®ä¾‹ç±»åˆ«ï¼šæ’åºå¹¶é™åˆ¶æ•°é‡
        selected_objects = []
        for class_name, objs in objects_by_class.items():
            if class_name in multi_instance_classes:
                # æŒ‰ç½®ä¿¡åº¦é™åºæ’åºï¼Œå–å‰ N ä¸ª
                objs_sorted = sorted(objs, key=lambda x: x['confidence'], reverse=True)
                selected_objects.extend(objs_sorted[:multi_max_count])
                print(f"  {class_name}: {len(objs_sorted[:multi_max_count])} instances (from {len(objs)} detected, conf>={multi_conf_threshold})")
            else:
                # å•å®ä¾‹ç±»åˆ«
                selected_objects.append(objs)
                print(f"  {class_name}: 1 instance (best)")
        
        print(f"\nTotal objects to segment: {len(selected_objects)}")
    
        # æ­¥éª¤3: å¯¹ç­›é€‰åçš„ç‰©ä½“è¿›è¡Œåˆ†å‰²
        print(f"\nStarting segmentation...")
        
        detected_objects = []
        for info in selected_objects:
            # è·å–åŸå§‹æ£€æµ‹æ¡†
            original_bbox = det_bboxes.xyxy[info['idx']]
            
            # æ‰©å±•æ£€æµ‹æ¡†ç”¨äºåˆ†å‰²
            expanded_bbox = self._expand_bbox(original_bbox, 40, image)
            
            # æ‰§è¡Œåˆ†å‰²
            seg_result = self.segment(image, original_bbox, output_dir, save_result=save_result, expand_pixels=40)
            
            obj = {
                'class': info['class_name'],
                'confidence': info['confidence'],
                'bbox_xyxy': original_bbox.cpu().numpy().astype(int).tolist(),  # åŸå§‹æ£€æµ‹æ¡†
                'bbox_xyxy_expanded': expanded_bbox.cpu().numpy().astype(int).tolist(),  # æ‰©å±•åçš„æ£€æµ‹æ¡†
                'mask': seg_result['mask']
            }
            detected_objects.append(obj)
            print(f"  Segmented: {info['class_name']} (conf={info['confidence']:.2f})")
        
        print(f"\nTotal objects processed: {len(detected_objects)}")
        
        # æ­¥éª¤4: åˆ›å»ºåˆå¹¶çš„å¯è§†åŒ–å›¾åƒï¼ˆæ£€æµ‹æ¡† + åˆ†å‰²æ©ç ï¼‰
        combined_path = None
        if save_result and len(detected_objects) > 0:
            combined_path = self._create_combined_visualization(
                image, detected_objects, output_dir
            )
        
        return {
            'success': True,
            'objects': detected_objects,
            'detection_path': det_result['detection_path'],
            'combined_path': combined_path
        }
    
    def _create_combined_visualization(self, image, objects, output_dir):
        """
        åˆ›å»ºåˆå¹¶çš„å¯è§†åŒ–å›¾åƒï¼ˆæ£€æµ‹æ¡† + åˆ†å‰²æ©ç ï¼‰
        
        å‚æ•°:
            image: å›¾åƒè·¯å¾„æˆ–æ•°ç»„
            objects: æ£€æµ‹åˆ°çš„ç‰©ä½“åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        è¿”å›:
            combined_path: åˆå¹¶å›¾åƒçš„ä¿å­˜è·¯å¾„
        """
        # è¯»å–åŸå§‹å›¾åƒ
        if isinstance(image, str):
            img = cv2.imread(image)
        elif isinstance(image, np.ndarray):
            img = image.copy()
        else:
            img = np.array(image)
        
        if len(img.shape) == 2:  # ç°åº¦å›¾
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        
        # åˆ›å»ºæ©ç å åŠ å±‚
        overlay = img.copy()
        
        # ä¸ºä¸åŒç‰©ä½“å®šä¹‰é¢œè‰²ï¼ˆBGRæ ¼å¼ï¼‰
        colors = [
            (255, 0, 0),      # è“è‰²
            (0, 255, 0),      # ç»¿è‰²
            (0, 0, 255),      # çº¢è‰²
            (255, 255, 0),    # é’è‰²
            (255, 0, 255),    # ç´«è‰²
            (0, 255, 255),    # é»„è‰²
            (128, 0, 255),    # ç²‰è‰²
            (0, 128, 255),    # æ©™è‰²
        ]
        
        for idx, obj in enumerate(objects):
            color = colors[idx % len(colors)]
            
            # 1. ç»˜åˆ¶åˆ†å‰²æ©ç ï¼ˆåŠé€æ˜å¡«å……ï¼‰
            if obj['mask'] is not None:
                mask = obj['mask']
                if mask.shape[:2] != img.shape[:2]:
                    # å¦‚æœæ©ç å°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´å¤§å°
                    mask = cv2.resize(mask.astype(np.uint8), 
                                     (img.shape[1], img.shape[0]), 
                                     interpolation=cv2.INTER_NEAREST)
                
                # åˆ›å»ºå½©è‰²æ©ç 
                colored_mask = np.zeros_like(img)
                colored_mask[mask > 0] = color
                
                # å åŠ æ©ç ï¼ˆé€æ˜åº¦0.4ï¼‰
                overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.4, 0)
            
            # 2. ç»˜åˆ¶åŸå§‹æ£€æµ‹æ¡†ï¼ˆç»†çº¿ï¼‰
            bbox = obj['bbox_xyxy']
            x1, y1, x2, y2 = map(int, bbox)
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 2)
            
            # 3. ç»˜åˆ¶æ‰©å±•åçš„æ£€æµ‹æ¡†ï¼ˆè™šçº¿ï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if 'bbox_xyxy_expanded' in obj:
                bbox_exp = obj['bbox_xyxy_expanded']
                x1_exp, y1_exp, x2_exp, y2_exp = map(int, bbox_exp)
                # ç»˜åˆ¶è™šçº¿çŸ©å½¢ï¼ˆç”¨å¤šä¸ªå°çº¿æ®µæ¨¡æ‹Ÿï¼‰
                self._draw_dashed_rectangle(overlay, (x1_exp, y1_exp), (x2_exp, y2_exp), (0, 255, 255), 2)
            
            # 4. ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            label = f"{obj['class']} {obj['confidence']:.2f}"
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            # æ ‡ç­¾èƒŒæ™¯æ¡†
            cv2.rectangle(
                overlay,
                (x1, y1 - text_height - baseline - 5),
                (x1 + text_width + 5, y1),
                color,
                -1  # å¡«å……
            )
            
            # 5. ç»˜åˆ¶æ ‡ç­¾æ–‡å­—ï¼ˆç™½è‰²ï¼‰
            cv2.putText(
                overlay,
                label,
                (x1 + 2, y1 - baseline - 2),
                font,
                font_scale,
                (255, 255, 255),  # ç™½è‰²
                thickness
            )
        
        # ä¿å­˜åˆå¹¶å›¾åƒ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        combined_filename = f"combined_{timestamp}.jpg"
        combined_path = os.path.join(output_dir, combined_filename)
        cv2.imwrite(combined_path, overlay)
        
        print(f"\nğŸ’¾ åˆå¹¶å¯è§†åŒ–å·²ä¿å­˜: {combined_path}")
        
        return combined_path

if __name__ == "__main__":
        # ä½ çš„å›¾ç‰‡è·¯å¾„ (è¯·ç¡®ä¿å›¾ç‰‡å­˜åœ¨)
    image_to_process = "./images/cup2.jpg"  # <--- åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„å›¾ç‰‡è·¯å¾„ï¼

    # ä½ æƒ³è¦æ£€æµ‹çš„ç‰©ä½“ç±»åˆ« (YOLO-World å¯¹è‡ªç„¶è¯­è¨€æè¿°æ›´å‹å¥½)
    categories_to_find = ["cup"]

    segmentator = YOLOSegmentator()

    # --- æ‰§è¡Œæ£€æµ‹å’Œåˆ†å‰² ---
    result = segmentator.detect_and_segment(
        image=image_to_process,
        categories=categories_to_find
    )
