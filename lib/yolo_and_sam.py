import torch
from ultralytics import YOLOWorld, FastSAM
import numpy as np
import time
import os
import cv2
from PIL import Image
from functools import wraps


def timer(func):
    """
    è®¡æ—¶è£…é¥°å™¨ï¼šç»Ÿè®¡å‡½æ•°æ‰§è¡Œæ—¶é—´
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # è·å–å‡½æ•°å
        func_name = func.__name__
        print(f"â±ï¸  [{func_name}] took {elapsed_time:.3f} seconds")
        
        return result
    return wrapper


class YOLOSegmentator:
    def __init__(self, yolo_weights="weights/yolov8s-world.pt", sam_weights="weights/FastSAM-s.pt"):
        """
        æ­£ç¡®åˆå§‹åŒ– YOLO-World å’Œ FastSAM æ¨¡å‹ã€‚
        """
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")

        # --- æ­£ç¡®çš„ YOLO-World åˆå§‹åŒ–æµç¨‹ ---

        # 1. åŠ è½½æ¨¡å‹ (é»˜è®¤åœ¨ CPU ä¸Š)
        print("Loading YOLO-World model ...")
        self.det_model = YOLOWorld(yolo_weights)
        self.det_model.to(self.device)

        # åŠ è½½ FastSAM æ¨¡å‹
        print("Loading FastSAM model...")
        self.seg_model = FastSAM(sam_weights)
        self.seg_model.to(self.device)

    @timer
    def detect(self, image, categories, output_dir="result", conf=0.1, imgsz=640, save_result=True):
        """
        ä½¿ç”¨ YOLO-World æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“
        
        å‚æ•°:
            image: å›¾åƒæ•°æ® (numpy æ•°ç»„æ ¼å¼)/å›¾åƒè·¯å¾„
            categories: è¦æ£€æµ‹çš„ç±»åˆ«åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: å›¾åƒå¤§å°
            
        è¿”å›:
            dict: {
                'success': bool,
                'det_bboxes': YOLO result object (åŒ…å«boxes, cls, confç­‰),
                'detection_path': str (ä¿å­˜çš„æ£€æµ‹ç»“æœè·¯å¾„)
            }
        """

        print("\n--- Running YOLO-World Detection ---")

        self.det_model.set_classes(categories)

        if type(image) == str:
            pass
        elif isinstance(image, list):
            image = Image.fromarray(image)
        #å¦‚æœpredictå¤šä¸ªimgï¼Œå¯¹äºæœ‰å¤šä¸ªimgçš„ç»“æœ.list
        det_results = self.det_model.predict(
            source=image,
            conf=conf,
            imgsz=imgsz,
            device=self.device,
            verbose=False
        )

        #det_resultsåŒ…å«äº†å¤§é‡çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬.boxes(xyxy,cls,conf),orig_img,names,speedç­‰ç­‰
        result = det_results[0]

        #é”™è¯¯æç¤º
        if len(result.boxes) == 0:
            print("No objects detected by YOLO-World.")
            return {'success': False, 'det_bboxes': None, 'detection_path': None}

        #ä¿å­˜ç»“æœ
        if save_result:
             # ä¿å­˜æ£€æµ‹ç»“æœï¼ˆå¸¦æ ‡ç­¾çš„æ£€æµ‹æ¡†ï¼‰
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            det_output_filename = os.path.join(output_dir, f"det_{os.path.basename(image) if isinstance(image, str) else f'{timestamp}.png'}")
            result.save(det_output_filename)
            print('YOLO-World results:')
            print(f"Detection result saved to {det_output_filename}")
            print(f"Found {len(result.boxes)} objects")

        result_dict = {
            'success': True,
            'det_bboxes': result.boxes,
            'detection_path': det_output_filename if save_result else None
        }

        return result_dict

    @timer
    def segment(self, image, bbox_gpu, output_dir, save_result):
        """
        ä½¿ç”¨ FastSAM å¯¹æŒ‡å®šè¾¹ç•Œæ¡†è¿›è¡Œåˆ†å‰²
        
        å‚æ•°:
            image: å›¾åƒæ•°æ® (numpy æ•°ç»„æ ¼å¼) | å›¾ç‰‡åœ°å€
            bbox_gpu: è¾¹ç•Œæ¡†å¼ é‡ (åœ¨GPUä¸Šï¼Œæ ¼å¼ä¸º xyxy)
            output_dir: è¾“å‡ºç›®å½•
            save_result: æ˜¯å¦ä¿å­˜åˆ†å‰²ç»“æœ
            
        è¿”å›:
            dict: {
                'success': bool,
                'mask': np.ndarray (åˆ†å‰²æ©ç ),
                'segmentation_path': str (å¦‚æœä¿å­˜äº†ç»“æœ)
            }
        """

        sam_results = self.seg_model(image, bboxes=bbox_gpu.unsqueeze(0))


        # æå–æ©ç æ•°æ®
        mask = None
        if sam_results[0].masks is not None and len(sam_results[0].masks) > 0:
            mask = sam_results[0].masks.data[0].cpu().numpy().astype(np.uint8)


        # ä¿å­˜åˆ†å‰²ç»“æœ
        if save_result:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            seg_output_filename = os.path.join(output_dir, f"seg_{os.path.basename(image) if isinstance(image, str) else f'{timestamp}.png'}")
            sam_results[0].save(seg_output_filename)
            print('FastSAM results:')
            print(f"Segmentation result saved to {seg_output_filename}")

        result_dict = {
            'success': True,
            'mask': mask,
            'segmentation_path': seg_output_filename if save_result else None
        }

        return result_dict

    def detect_and_segment(self, image, categories, output_dir="./result", conf=0.1, imgsz=640, save_result=True):
        """
        å…ˆç”¨ YOLO-World æ£€æµ‹ï¼Œç„¶åç”¨ FastSAM åˆ†å‰²ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡ã€‚
        **åªåˆ†å‰²å…¨å±€ç½®ä¿¡åº¦æœ€é«˜çš„1ä¸ªç‰©ä½“
        """
        if save_result:
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
        # æ­¥éª¤1: æ£€æµ‹
        det_result = self.detect(image, categories, output_dir, conf, imgsz)
        
        if not det_result['success']:
            return det_result
        
        det_bboxes = det_result['det_bboxes']
        

        # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ªç‰©ä½“-------------------------------------------------------
        max_index_gpu = det_bboxes.conf.argmax()
        best_box_gpu = det_bboxes.xyxy[max_index_gpu]
        best_class_id_gpu = det_bboxes.cls[max_index_gpu]
        best_confidence_gpu = det_bboxes.conf[max_index_gpu]

        # å°†ç»“æœç§»è‡³ CPU
        best_box_xyxy = best_box_gpu.cpu().numpy().astype(int)
        best_class_name = self.det_model.names[int(best_class_id_gpu.cpu())]
        best_confidence = float(best_confidence_gpu.cpu())

        #-------------------------------------------------------------------------------------------

        print(f"\nBest object: {best_class_name} (confidence: {best_confidence:.2f})")

        # æ­¥éª¤2: åˆ†å‰²
        seg_result = self.segment(image, best_box_gpu, output_dir, save_result=save_result)

        return {
            'success': True,
            'best_object': {
                'class': best_class_name,
                'confidence': best_confidence,
                'bbox_xyxy': best_box_xyxy.tolist()
            },
            'mask': seg_result['mask'],
            'detection_path': det_result['detection_path'] if save_result else None,
            'segmentation_path': seg_result.get('segmentation_path') if save_result else None
        }
    
    def detect_and_segment_all(self, image, categories, output_dir="./result", conf=0.1, imgsz=640, save_result=False, 
                               multi_instance_classes=None, multi_conf_threshold=0.3, multi_max_count=3):
        """
        æ£€æµ‹å¹¶åˆ†å‰²æ‰€æœ‰æŒ‡å®šç±»åˆ«çš„ç‰©ä½“
        
        å‚æ•°:
            image: å›¾åƒè·¯å¾„æˆ–æ•°ç»„
            categories: è¦æ£€æµ‹çš„ç±»åˆ«åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            conf: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: å›¾åƒå¤§å°
            save_result: æ˜¯å¦ä¿å­˜ç»“æœ
            multi_instance_classes: å…è®¸ä¿ç•™å¤šä¸ªå®ä¾‹çš„ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚ ['bowl']ï¼‰
            multi_conf_threshold: å¤šå®ä¾‹ç±»åˆ«çš„ç½®ä¿¡åº¦é˜ˆå€¼
            multi_max_count: å¤šå®ä¾‹ç±»åˆ«çš„æœ€å¤§ä¿ç•™æ•°é‡
        
        è¿”å›:
            dict: {
                'success': bool,
                'objects': [
                    {
                        'class': str,
                        'confidence': float,
                        'bbox_xyxy': list,
                        'mask': np.ndarray
                    },
                    ...
                ]
            }
        """
        if save_result:
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
        
        if multi_instance_classes is None:
            multi_instance_classes = []
            
        # æ­¥éª¤1: æ£€æµ‹
        det_result = self.detect(image, categories, output_dir, conf, imgsz)
        
        if not det_result['success']:
            return det_result
        
        det_bboxes = det_result['det_bboxes']

        # æ­¥éª¤2: ç­›é€‰ç‰©ä½“
        print(f"\nFound {len(det_bboxes)} objects, filtering...")

        # åˆ›å»º (idx, class_name, confidence) çš„åˆ—è¡¨å¹¶æŒ‰ç±»åˆ«åˆ†ç»„
        objects_by_class = {}
        for idx in range(len(det_bboxes)):
            info = {
                'idx': idx,
                'class_name': self.det_model.names[int(det_bboxes.cls[idx].cpu())],
                'confidence': float(det_bboxes.conf[idx].cpu())
            }
            class_name = info['class_name']
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯å¤šå®ä¾‹ç±»åˆ«
            if class_name in multi_instance_classes:
                # å¤šå®ä¾‹ç±»åˆ«ï¼šä¿ç•™æ‰€æœ‰æ»¡è¶³é˜ˆå€¼çš„ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
                if class_name not in objects_by_class:
                    objects_by_class[class_name] = []
                if info['confidence'] >= multi_conf_threshold:
                    objects_by_class[class_name].append(info)
            else:
                # å•å®ä¾‹ç±»åˆ«ï¼šåªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
                if class_name not in objects_by_class or info['confidence'] > objects_by_class[class_name]['confidence']:
                    objects_by_class[class_name] = info
        
        # å¤„ç†å¤šå®ä¾‹ç±»åˆ«ï¼šæ’åºå¹¶é™åˆ¶æ•°é‡
        selected_objects = []
        for class_name, objs in objects_by_class.items():
            if class_name in multi_instance_classes:
                # æŒ‰ç½®ä¿¡åº¦é™åºæ’åºï¼Œå–å‰ N ä¸ª
                objs_sorted = sorted(objs, key=lambda x: x['confidence'], reverse=True)
                selected_objects.extend(objs_sorted[:multi_max_count])
                print(f"  {class_name}: {len(objs_sorted[:multi_max_count])} instances (from {len(objs)} detected, conf>={multi_conf_threshold})")
            else:
                # å•å®ä¾‹ç±»åˆ«
                selected_objects.append(objs)
                print(f"  {class_name}: 1 instance (best)")
        
        print(f"\nTotal objects to segment: {len(selected_objects)}")
    
        # æ­¥éª¤3: å¯¹ç­›é€‰åçš„ç‰©ä½“è¿›è¡Œåˆ†å‰²
        print(f"\nStarting segmentation...")
        
        detected_objects = []
        for info in selected_objects:
            obj = {
                'class': info['class_name'],
                'confidence': info['confidence'],
                'bbox_xyxy': det_bboxes.xyxy[info['idx']].cpu().numpy().astype(int).tolist(),
                'mask': self.segment(image, det_bboxes.xyxy[info['idx']], output_dir, save_result=save_result)['mask']
            }
            detected_objects.append(obj)
            print(f"  Segmented: {info['class_name']} (conf={info['confidence']:.2f})")
        
        print(f"\nTotal objects processed: {len(detected_objects)}")
        
        # æ­¥éª¤4: åˆ›å»ºåˆå¹¶çš„å¯è§†åŒ–å›¾åƒï¼ˆæ£€æµ‹æ¡† + åˆ†å‰²æ©ç ï¼‰
        combined_path = None
        if save_result and len(detected_objects) > 0:
            combined_path = self._create_combined_visualization(
                image, detected_objects, output_dir
            )
        
        return {
            'success': True,
            'objects': detected_objects,
            'detection_path': det_result['detection_path'],
            'combined_path': combined_path
        }
    
    def _create_combined_visualization(self, image, objects, output_dir):
        """
        åˆ›å»ºåˆå¹¶çš„å¯è§†åŒ–å›¾åƒï¼ˆæ£€æµ‹æ¡† + åˆ†å‰²æ©ç ï¼‰
        
        å‚æ•°:
            image: å›¾åƒè·¯å¾„æˆ–æ•°ç»„
            objects: æ£€æµ‹åˆ°çš„ç‰©ä½“åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        è¿”å›:
            combined_path: åˆå¹¶å›¾åƒçš„ä¿å­˜è·¯å¾„
        """
        # è¯»å–åŸå§‹å›¾åƒ
        if isinstance(image, str):
            img = cv2.imread(image)
        elif isinstance(image, np.ndarray):
            img = image.copy()
        else:
            img = np.array(image)
        
        if len(img.shape) == 2:  # ç°åº¦å›¾
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        
        # åˆ›å»ºæ©ç å åŠ å±‚
        overlay = img.copy()
        
        # ä¸ºä¸åŒç‰©ä½“å®šä¹‰é¢œè‰²ï¼ˆBGRæ ¼å¼ï¼‰
        colors = [
            (255, 0, 0),      # è“è‰²
            (0, 255, 0),      # ç»¿è‰²
            (0, 0, 255),      # çº¢è‰²
            (255, 255, 0),    # é’è‰²
            (255, 0, 255),    # ç´«è‰²
            (0, 255, 255),    # é»„è‰²
            (128, 0, 255),    # ç²‰è‰²
            (0, 128, 255),    # æ©™è‰²
        ]
        
        for idx, obj in enumerate(objects):
            color = colors[idx % len(colors)]
            
            # 1. ç»˜åˆ¶åˆ†å‰²æ©ç ï¼ˆåŠé€æ˜å¡«å……ï¼‰
            if obj['mask'] is not None:
                mask = obj['mask']
                if mask.shape[:2] != img.shape[:2]:
                    # å¦‚æœæ©ç å°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´å¤§å°
                    mask = cv2.resize(mask.astype(np.uint8), 
                                     (img.shape[1], img.shape[0]), 
                                     interpolation=cv2.INTER_NEAREST)
                
                # åˆ›å»ºå½©è‰²æ©ç 
                colored_mask = np.zeros_like(img)
                colored_mask[mask > 0] = color
                
                # å åŠ æ©ç ï¼ˆé€æ˜åº¦0.4ï¼‰
                overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.4, 0)
            
            # 2. ç»˜åˆ¶æ£€æµ‹æ¡†
            bbox = obj['bbox_xyxy']
            x1, y1, x2, y2 = map(int, bbox)
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 2)
            
            # 3. ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            label = f"{obj['class']} {obj['confidence']:.2f}"
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            # æ ‡ç­¾èƒŒæ™¯æ¡†
            cv2.rectangle(
                overlay,
                (x1, y1 - text_height - baseline - 5),
                (x1 + text_width + 5, y1),
                color,
                -1  # å¡«å……
            )
            
            # 4. ç»˜åˆ¶æ ‡ç­¾æ–‡å­—ï¼ˆç™½è‰²ï¼‰
            cv2.putText(
                overlay,
                label,
                (x1 + 2, y1 - baseline - 2),
                font,
                font_scale,
                (255, 255, 255),  # ç™½è‰²
                thickness
            )
        
        # ä¿å­˜åˆå¹¶å›¾åƒ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        combined_filename = f"combined_{timestamp}.jpg"
        combined_path = os.path.join(output_dir, combined_filename)
        cv2.imwrite(combined_path, overlay)
        
        print(f"\nğŸ’¾ åˆå¹¶å¯è§†åŒ–å·²ä¿å­˜: {combined_path}")
        
        return combined_path

if __name__ == "__main__":
        # ä½ çš„å›¾ç‰‡è·¯å¾„ (è¯·ç¡®ä¿å›¾ç‰‡å­˜åœ¨)
    image_to_process = "./images/cup2.jpg"  # <--- åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„å›¾ç‰‡è·¯å¾„ï¼

    # ä½ æƒ³è¦æ£€æµ‹çš„ç‰©ä½“ç±»åˆ« (YOLO-World å¯¹è‡ªç„¶è¯­è¨€æè¿°æ›´å‹å¥½)
    categories_to_find = ["cup"]

    segmentator = YOLOSegmentator()

    # --- æ‰§è¡Œæ£€æµ‹å’Œåˆ†å‰² ---
    result = segmentator.detect_and_segment(
        image=image_to_process,
        categories=categories_to_find
    )
