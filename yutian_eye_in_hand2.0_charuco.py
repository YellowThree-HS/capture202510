import numpy as np
import cv2
import cv2.aruco as aruco
import os
import csv
# import pupil_apriltags as apriltag
import transforms3d as tfs
import math
import random
import time


from lib.dobot import DobotRobot
from lib.camera import Camera



def convert_euler_to_rotation_matrix(x, y, z, rx, ry, rz):
    """
    Convert Euler angles to a rotation matrix.
    Args:
    x, y, z: Translation coordinates.
    rx, ry, rz: Rotation angles in radians.

    Returns:
    A 4x4 rotation matrix.
    """
    rx = rx / math.pi * 180
    ry = ry / math.pi * 180
    rz = rz / math.pi * 180
    rmat = tfs.euler.euler2mat(math.radians(rx), math.radians(ry), math.radians(rz))
    rotation_matrix = tfs.affines.compose(np.squeeze(np.asarray((x, y, z))), rmat, [1, 1, 1])
    return rotation_matrix


def rotation_matrix_to_euler_angles(R):
    """
    Convert a rotation matrix to Euler angles.
    Args:
    R: A 3x3 rotation matrix.

    Returns:
    A tuple of Euler angles (phi, theta, psi).
    """
    sy = math.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])
    singular = sy < 1e-6

    if not singular:
        x = math.atan2(R[2, 1], R[2, 2])
        y = math.atan2(-R[2, 0], sy)
        z = math.atan2(R[1, 0], R[0, 0])
    else:
        x = math.atan2(-R[1, 2], R[1, 1])
        y = math.atan2(-R[2, 0], sy)
        z = 0

    return np.array([x, y, z])

def hand_eye_calibration(
    R_end2base, 
    t_end2base, 
    R_board2camera, 
    t_board2camera, 
    method=cv2.CALIB_HAND_EYE_PARK
):
    """M_board2base
    æ‰‹çœ¼æ ‡å®šå°è£…å‡½æ•°
    
    å‚æ•°:
        R_end2base (list): æœºæ¢°è‡‚æœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬çŸ©é˜µåˆ—è¡¨ [R1, R2, ...] (æ¯ä¸ªä¸º3x3 np.array)
        t_end2base (list): æœºæ¢°è‡‚æœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»å‘é‡åˆ—è¡¨ [t1, t2, ...] (æ¯ä¸ªä¸º3x1 np.array)
        R_board2camera (list): æ ‡å®šæ¿åˆ°ç›¸æœºçš„æ—‹è½¬çŸ©é˜µåˆ—è¡¨
        t_board2camera (list): æ ‡å®šæ¿åˆ°ç›¸æœºçš„å¹³ç§»å‘é‡åˆ—è¡¨
        method: æ ‡å®šæ–¹æ³• (é»˜è®¤PARKç®—æ³•)
        M_board2base_list (list): æ ‡å®šæ¿åˆ°åŸºåº§çš„é½æ¬¡å˜æ¢çŸ©é˜µåˆ—è¡¨ (4x4 np.array)
        
    è¿”å›:
        calibrate_mean (np.array): æ ‡å®šç»“æœçš„å‡å€¼ (4x4 é½æ¬¡çŸ©é˜µ)
        calibrate_std (np.array): æ ‡å®šç»“æœçš„æ ‡å‡†å·® (4x4 é½æ¬¡çŸ©é˜µ)
        M_cam2end (np.array): ç›¸æœºåˆ°æœ«ç«¯çš„é½æ¬¡å˜æ¢çŸ©é˜µ (4x4)
    """
    # 1. æ‰§è¡Œæ‰‹çœ¼æ ‡å®š
    R_cam2end, t_cam2end = cv2.calibrateHandEye(
        R_gripper2base=R_end2base,
        t_gripper2base=t_end2base,
        R_target2cam=R_board2camera,
        t_target2cam=t_board2camera,
        method=method
    )
    
    # 2. æ„å»ºç›¸æœºåˆ°æœ«ç«¯çš„é½æ¬¡å˜æ¢çŸ©é˜µ
    M_cam2end = np.eye(4)
    M_cam2end[:3, :3] = R_cam2end
    M_cam2end[:3, 3] = t_cam2end.reshape(3)

    M_end2base=np.eye(4)
    M_board2camera=np.eye(4)
    M_board2base=[]

    for sub_R_end2base,sub_t_end2base,sub_R_board2camera,sub_t_board2camera in zip(R_end2base, t_end2base, R_board2camera, t_board2camera):
        M_end2base[:3, :3] =sub_R_end2base
        M_end2base[:3, 3] =sub_t_end2base.reshape(3)
        M_board2camera[:3, :3] =sub_R_board2camera
        M_board2camera[:3, 3] =sub_t_board2camera.reshape(3)
        M_board2base.append(M_end2base@M_cam2end@M_board2camera)

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæ³¨æ„ï¼šé½æ¬¡çŸ©é˜µçš„ç»Ÿè®¡éœ€åˆ†ç¦»æ—‹è½¬å’Œå¹³ç§»ï¼‰
    calibrate_mean = np.mean(M_board2base, axis=0)
    calibrate_std = np.std([np.linalg.norm(m[:3, 3]) for m in M_board2base])
    

    return calibrate_mean, calibrate_std, M_cam2end


def enhance_image_for_detection(image):
    """
    å›¾åƒé¢„å¤„ç†ï¼Œæé«˜ AprilTag æ£€æµ‹ç¨³å®šæ€§
    """
    # 1. è½¬ç°åº¦
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()
    
    # 2. ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆæ”¹å–„å…‰ç…§ï¼‰
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    
    # 3. åŒè¾¹æ»¤æ³¢ï¼ˆä¿ç•™è¾¹ç¼˜çš„åŒæ—¶é™å™ªï¼‰
    denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)
    
    # 4. é”åŒ–ï¼ˆå¯é€‰ï¼Œå¢å¼ºè¾¹ç¼˜ï¼‰
    kernel = np.array([[-1,-1,-1],
                       [-1, 9,-1],
                       [-1,-1,-1]])
    sharpened = cv2.filter2D(denoised, -1, kernel)
    
    return sharpened


def main(use_camera=True):
    """
    Main function to run the camera and robot control loop.
    """
    np.set_printoptions(suppress=True)

    # cam=OrbbecCamera.OrbbecCamera(device_index=0)
    cam =  Camera(camera_model='d405')
    camera_matrix = cam.get_camera_matrix('color')
    distortion_coefficients = cam.get_distortion_coeffs('color')
    image_id = 0
    calibration_path = './Calibration_Pic/'
    os.makedirs(calibration_path, exist_ok=True)
    robot = DobotRobot("192.168.5.1", no_gripper=True)
    
    # è¿åŠ¨åˆ°åˆå§‹ä½å§¿
    init_joint_positions = np.array([-90.0, 0.0, -90.0, 0.0, 90.0, 90.0, 1.0])
    robot.moveJ(init_joint_positions)
    
    # åˆå§‹åŒ–å‚æ•°
    # squaresX = 14
    # squaresY = 9
    # squareLength = 0.02
    # markerLength = 0.015
    # aruco_type = cv2.aruco.DICT_5X5_1000
    
    # åˆ›å»ºå­—å…¸å’Œæ ‡å®šæ¿å¯¹è±¡
    # dictionary = cv2.aruco.getPredefinedDictionary(aruco_type)
    # board = cv2.aruco.CharucoBoard((squaresX, squaresY), squareLength, markerLength, dictionary)
    # aruco_detector = cv2.aruco.ArucoDetector(dictionary, aruco_params)
    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_APRILTAG_36H11)
    aruco_params = cv2.aruco.DetectorParameters()

    # Lists to store transformation data
    robot_transforms, camera_transforms = [], []
    R_end2base, t_end2base, R_board2camera, t_board2camera = [], [], [], []
    M_board2base_list = []
    M_cam2end = np.eye(4)
    method={
        "TSAI":cv2.CALIB_HAND_EYE_TSAI,
        "PARK":cv2.CALIB_HAND_EYE_PARK,
        "DANIILIDIS":cv2.CALIB_HAND_EYE_DANIILIDIS,
        "HORAUD":cv2.CALIB_HAND_EYE_HORAUD,
    }
    
    # ==================== é”®ç›˜æ§åˆ¶å‚æ•° ====================
    # ç§»åŠ¨æ­¥é•¿ï¼ˆæ¯«ç±³å’Œåº¦ï¼‰
    pos_step = 50.0   # ä½ç½®æ­¥é•¿ 10mm
    rot_step = 10.0    # æ—‹è½¬æ­¥é•¿ 5åº¦
    pos_step_fine = 1.0   # ç²¾ç»†ä½ç½®æ­¥é•¿ 1mm
    rot_step_fine = 1.0   # ç²¾ç»†æ—‹è½¬æ­¥é•¿ 1åº¦
    
    fine_mode = False  # ç²¾ç»†è°ƒæ•´æ¨¡å¼
    
    print("\n" + "="*70)
    print("ğŸ® é”®ç›˜æ§åˆ¶æ‰‹çœ¼æ ‡å®šç¨‹åº")
    print("="*70)
    print("\nğŸ“‹ æ§åˆ¶è¯´æ˜:")
    print("  ä½ç½®æ§åˆ¶ (ç›¸å¯¹äºå½“å‰ä½ç½®):")
    print("    â†‘/â†“    : Yè½´ å‰è¿›/åé€€")
    print("    â†/â†’    : Xè½´ å·¦ç§»/å³ç§»")
    print("    W/S    : Zè½´ ä¸Šå‡/ä¸‹é™")
    print("\n  å§¿æ€æ§åˆ¶:")
    print("    Q/E    : ç»•Zè½´æ—‹è½¬ (Rz)")
    print("    A/D    : ç»•Xè½´æ—‹è½¬ (Rx)")
    print("    Z/C    : ç»•Yè½´æ—‹è½¬ (Ry)")
    print("\n  åŠŸèƒ½é”®:")
    print("    Space  : é‡‡é›†å½“å‰ä½å§¿æ•°æ®")
    print("    R      : å›åˆ°åˆå§‹ä½å§¿")
    print("    F      : åˆ‡æ¢ç²¾ç»†/ç²—è°ƒæ¨¡å¼ (å½“å‰: ç²—è°ƒ)")
    print("    P      : æ˜¾ç¤ºå½“å‰ä½å§¿")
    print("    H      : æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print("    ESC    : é€€å‡ºç¨‹åº")
    print("="*70 + "\n")
    
    try:
        count = 0
        while True:
            color_image, _ = cam.get_frames()

            undistorted_image = cv2.undistort(color_image, camera_matrix, distortion_coefficients)
            # marker_corners, marker_ids, rejected = aruco_detector.detectMarkers(undistorted_image)
            gray = enhance_image_for_detection(color_image)
            # cv2.imshow('gray',gray)
            marker_corners, marker_ids, rejected = aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)

            image_copy = color_image.copy()
            
            markerLength = 0.08 # ç¤ºä¾‹ï¼šå‡è®¾æ ‡è®°çš„è¾¹é•¿ä¸º 9 å˜ç±³ (0.05ç±³)
            rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(marker_corners, markerLength, camera_matrix, distortion_coefficients)
            # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„ marker
            if marker_ids is not None:
                for i in range(len(marker_ids)):
                    rvec = rvecs[i][0]
                    
                    tvec = tvecs[i][0]
                    cv2.drawFrameAxes(image_copy, camera_matrix, distortion_coefficients, rvec, tvec, 0.05) # è½´çš„é•¿åº¦ä¸º 5cm


            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow('Hand-Eye Calibration (Press H for help)', image_copy)
            
            key = cv2.waitKey(1) & 0xFF
            # ==================== é”®ç›˜æ§åˆ¶ ====================
            if key == 255:  # æ— æŒ‰é”®
                continue
                
            # è·å–å½“å‰ä½å§¿
            curr_pose = robot.get_XYZrxryrz_state()
            original_pose = curr_pose.copy()
            step_pos = pos_step_fine if fine_mode else pos_step
            step_rot = rot_step_fine if fine_mode else rot_step
            moved = False
            
            # ä½ç½®æ§åˆ¶
            if key == ord('i') or key == ord('I'):  # Y+ å‰è¿›
                curr_pose[1] += step_pos
                moved = True
                # print(f"I: Y+ {step_pos}mm")
            elif key == ord('k') or key == ord('K'):  # Y- åé€€
                curr_pose[1] -= step_pos
                moved = True
                # print(f"K: Y- {step_pos}mm")
            elif key == ord('j') or key == ord('J'):  # X- å·¦ç§»
                curr_pose[0] -= step_pos
                moved = True
                # print(f"J: X- {step_pos}mm")
            elif key == ord('l') or key == ord('L'):  # X+ å³ç§»
                curr_pose[0] += step_pos
                moved = True
                # print(f"L: X+ {step_pos}mm")
            elif key == ord('w') or key == ord('W'):  # Z+
                curr_pose[2] += step_pos
                moved = True
                # print(f"W Z+ {step_pos}mm")
            elif key == ord('s') or key == ord('S'):  # Z-
                curr_pose[2] -= step_pos
                moved = True
                # print(f"S Z- {step_pos}mm")
                
            # å§¿æ€æ§åˆ¶
            elif key == ord('q') or key == ord('Q'):  # Rz+
                curr_pose[5] += step_rot
                moved = True
                # print(f"Q Rz+ {step_rot}Â°")
            elif key == ord('e') or key == ord('E'):  # Rz-
                curr_pose[5] -= step_rot
                moved = True
                # print(f"E Rz- {step_rot}Â°")
            elif key == ord('a') or key == ord('A'):  # Rx+
                curr_pose[3] += step_rot
                moved = True
                # print(f"A Rx+ {step_rot}Â°")
            elif key == ord('d') or key == ord('D'):  # Rx-
                curr_pose[3] -= step_rot
                moved = True
                # print(f"D Rx- {step_rot}Â°")
            elif key == ord('z') or key == ord('Z'):  # Ry+
                curr_pose[4] += step_rot
                moved = True
                # print(f"Z Ry+ {step_rot}Â°")
            elif key == ord('c') or key == ord('C'):  # Ry-
                curr_pose[4] -= step_rot
                moved = True
                # print(f"C Ry- {step_rot}Â°")   
                
            # æ‰§è¡Œç§»åŠ¨
            if moved:
                try:
                    robot.moveL(curr_pose)
                    # time.sleep(0.5)  # ç­‰å¾…è¿åŠ¨å®Œæˆ
                except Exception as e:
                    print(f"âš ï¸ ç§»åŠ¨å¤±è´¥: {e}")
                    print("ğŸ”„ å°è¯•å›åˆ°åŸä½...")
                    robot.moveL(original_pose)
                    # time.sleep(0.5)
            
            # ==================== åŠŸèƒ½é”® ====================
            # é‡‡é›†æ•°æ®
            elif key == 32:  # Space
                image_save_path = "./collect_data/"
                cv2.namedWindow('detection', flags=cv2.WINDOW_NORMAL |
                                                cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_EXPANDED)
                cv2.imshow("detection", color_image)  # çª—å£æ˜¾ç¤ºï¼Œæ˜¾ç¤ºåä¸º Capture_Video

                k = cv2.waitKey(1) & 0xFF  # æ¯å¸§æ•°æ®å»¶æ—¶ 1msï¼Œå»¶æ—¶ä¸èƒ½ä¸º 0ï¼Œå¦åˆ™è¯»å–çš„ç»“æœä¼šæ˜¯é™æ€å¸§

                print(f"é‡‡é›†ç¬¬{count}ç»„æ•°æ®...")
                pose = robot.get_XYZrxryrz_state()  # è·å–å½“å‰æœºæ¢°è‡‚çŠ¶æ€ éœ€è¦æ ¹æ®å®é™…ä½¿ç”¨çš„æœºæ¢°è‡‚è·å¾—
                x, y, z, rx, ry, rz = pose
                rx, ry, rz = np.deg2rad([rx, ry, rz])
                pose = [x/1000.0, y/1000.0, z/1000.0, rx, ry, rz]
                print(f"æœºæ¢°è‡‚pose:{pose}")

                with open(f'{image_save_path}poses.txt', 'a+') as f:
                    # å°†åˆ—è¡¨ä¸­çš„å…ƒç´ ç”¨ç©ºæ ¼è¿æ¥æˆä¸€è¡Œ
                    pose_ = [str(i) for i in pose]
                    new_line = f'{",".join(pose_)}\n'
                    # å°†æ–°è¡Œé™„åŠ åˆ°æ–‡ä»¶çš„æœ«å°¾
                    f.write(new_line)

                cv2.imwrite(image_save_path + str(count) + '.jpg', color_image)
                count += 1
                # if rvec is None or tvec is None:
                #     print("âš ï¸ æœªæ£€æµ‹åˆ°æ ‡å®šæ¿ï¼Œè¯·è°ƒæ•´ä½å§¿åå†é‡‡é›†ï¼")
                #     continue
                
                # print(f"\n{'='*50}")
                # print(f"ğŸ“¸ é‡‡é›†ç¬¬ {image_id + 1} ç»„æ•°æ®...")
                
                # # ä¿å­˜æ•°æ®
                # R_charuco_to_camera, _ = cv2.Rodrigues(rvec)
                # M_board2camera = np.eye(4)
                # M_board2camera[:3, :3] = R_charuco_to_camera
                # M_board2camera[:3, 3] = tvec.flatten()
                
                # M_end2base = robot.get_pose_matrix()
                
                # R_board2camera.append(M_board2camera[:3, :3])
                # t_board2camera.append(M_board2camera[:3, 3])
                # R_end2base.append(M_end2base[:3, :3])
                # t_end2base.append(M_end2base[:3, 3])
                
                # print(f"âœ… å·²ä¿å­˜ç¬¬ {image_id + 1} ç»„æ•°æ®")
                # print(f"   æœ«ç«¯ä½ç½®: {M_end2base[:3, 3]}")
                # image_id += 1
                
                # # æ‰§è¡Œæ ‡å®šè®¡ç®—
                # if len(t_end2base) > 3:
                #     std_min = []
                #     M_list = []
                #     print(f"\nğŸ”§ æ‰§è¡Œæ‰‹çœ¼æ ‡å®š (å·²é‡‡é›† {len(t_end2base)} ç»„æ•°æ®)...")
                    
                #     for calib_name, calib_method in method.items():
                #         try:
                #             calib_mean, calib_std, M = hand_eye_calibration(
                #                 R_end2base, t_end2base, 
                #                 R_board2camera, t_board2camera, 
                #                 calib_method
                #             )
                #             print(f"   {calib_name:12s}: æ ‡å‡†å·® = {calib_std:.6f} mm")
                #             M_list.append(M)
                #             std_min.append(calib_std)
                #         except Exception as e:
                #             print(f"   {calib_name:12s}: è®¡ç®—å¤±è´¥ - {e}")
                #             M_list.append(None)
                #             std_min.append(float('inf'))
                    
                #     min_value = min(std_min)
                #     print(f"\n   æœ€ä½³æ–¹æ³•: {list(method.keys())[std_min.index(min_value)]}")
                #     print(f"   æœ€å°æ ‡å‡†å·®: {min_value:.6f} m")
                    
                #     if len(t_end2base) >= 10 and 0.000001 < min_value < 0.0015:
                #         print("\nğŸ‰ æ ‡å®šç²¾åº¦è¾¾æ ‡ï¼")
                #         break
                
                # print(f"{'='*50}\n")
            
            # å›åˆ°åˆå§‹ä½å§¿
            elif key == ord('r') or key == ord('R'):
                print("\nğŸ”„ å›åˆ°åˆå§‹ä½å§¿...")
                robot.moveJ(init_joint_positions)
                # time.sleep(2.0)
                print("âœ… å·²å›åˆ°åˆå§‹ä½å§¿\n")
            
            # åˆ‡æ¢ç²¾ç»†/ç²—è°ƒæ¨¡å¼
            elif key == ord('f') or key == ord('F'):
                fine_mode = not fine_mode
                mode = "ç²¾ç»†" if fine_mode else "ç²—è°ƒ"
                print(f"\nğŸ”§ åˆ‡æ¢åˆ°{mode}æ¨¡å¼")
                print(f"   ä½ç½®æ­¥é•¿: {pos_step_fine if fine_mode else pos_step}mm")
                print(f"   æ—‹è½¬æ­¥é•¿: {rot_step_fine if fine_mode else rot_step}Â°\n")
            
            # æ˜¾ç¤ºå½“å‰ä½å§¿
            elif key == ord('p') or key == ord('P'):
                curr_pose = robot.get_XYZrxryrz_state()
                print("\nğŸ“ å½“å‰ä½å§¿:")
                print(f"   ä½ç½®: X={curr_pose[0]:.2f}, Y={curr_pose[1]:.2f}, Z={curr_pose[2]:.2f} mm")
                print(f"   å§¿æ€: Rx={curr_pose[3]:.2f}, Ry={curr_pose[4]:.2f}, Rz={curr_pose[5]:.2f} Â°\n")
            
            # æ˜¾ç¤ºå¸®åŠ©
            elif key == ord('h') or key == ord('H'):
                print("\n" + "="*70)
                print("ğŸ“‹ é”®ç›˜æ§åˆ¶è¯´æ˜:")
                print("  ä½ç½®: â†‘â†“â†â†’ (XYå¹³é¢), W/S (Zè½´)")
                print("  å§¿æ€: Q/E (Rz), A/D (Rx), Z/C (Ry)")
                print("  åŠŸèƒ½: Space(é‡‡é›†) R(å›åŸä½) F(åˆ‡æ¢æ¨¡å¼) P(æ˜¾ç¤ºä½å§¿) ESC(é€€å‡º)")
                print("="*70 + "\n")
            
            # é€€å‡º
            elif key == 27:  # ESC
                print("\nâŒ ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
                break

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if len(t_end2base) > 0:
            print(f"\n{'='*70}")
            print("ğŸ’¾ ä¿å­˜æ ‡å®šç»“æœ...")
            print(std_min)
            min_value = min(std_min)
            calib_mean = list(method.keys())[std_min.index(min_value)]
            best_M = M_list[std_min.index(min_value)]
            print(f"æœ€ä½³æ ‡å®šæ–¹æ³•: {calib_mean}")
            print(f"æœ€å°æ ‡å‡†å·®: {min_value:.6f} mm")
            print("\nç›¸æœºåˆ°æœ«ç«¯çš„å˜æ¢çŸ©é˜µ:")
            print(best_M)
            np.savetxt("T_camera2end.txt", best_M)
            print("âœ… å·²ä¿å­˜åˆ° T_camera2end.txt")
            print(f"{'='*70}\n")
        
        cv2.destroyAllWindows()

def draw_axis_on_image(image, tag, rvec, tvec, camera_matrix, distortion_coefficients):
    """
    Draw the axis on the image based on the tag pose.
    """
    axis_points = np.float32([[0.05, 0, 0], [0, 0.05, 0], [0, 0, -0.05]]).reshape(-1, 3)
    img_points, _ = cv2.projectPoints(axis_points, rvec, tvec, camera_matrix, distortion_coefficients)

    center = tuple(tag.center.astype(int))
    image = cv2.line(image, center, tuple(img_points[0].ravel().astype(int)), (255, 0, 0), 3)
    image = cv2.line(image, center, tuple(img_points[1].ravel().astype(int)), (0, 255, 0), 3)
    image = cv2.line(image, center, tuple(img_points[2].ravel().astype(int)), (0, 0, 255), 3)
    return image

def save_calibration_data(path, camera_data, robot_data):
    """
    Save the calibration data to CSV files.
    """
    with open(os.path.join(path, 'camera_data.csv'), 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(camera_data)

    with open(os.path.join(path, 'robot_data.csv'), 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(robot_data)

if __name__ == "__main__":
    main(use_camera=True)